{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_gcs=pd.read_csv(\"gcs_scores_updated.csv\")\n",
    "d_gcs=d_gcs.drop(columns=['nursingchartcelltypecat','nursingchartcelltypevallabel'])\n",
    "d_gcs=d_gcs.drop_duplicates()\n",
    "d_gcs=d_gcs.rename(columns={'nursingchartentryoffset':'offset'})\n",
    "\n",
    "labs=pd.read_csv(\"labs_morevars.csv\")\n",
    "labs=labs.drop_duplicates()\n",
    "labs=labs.rename(columns={'labresultoffset':'offset'})\n",
    "\n",
    "cardiovas=pd.read_csv(\"drugate_norm_updated.csv\")\n",
    "cardiovas.drop_duplicates()\n",
    "cardiovas=cardiovas.drop(columns=['nursingchartvalue','SOFA_cardio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First merge\n",
    "labs_cardio=pd.merge(labs,cardiovas,how=\"outer\",on=['patientunitstayid','offset']).drop_duplicates()\n",
    "labs_cardio.to_csv(\"labs_cardio_interim.csv\",index=False)\n",
    "\n",
    "del labs\n",
    "del cardiovas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_build=pd.merge(labs_cardio,d_gcs,how=\"outer\",on=['patientunitstayid','offset']).drop_duplicates()\n",
    "del d_gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16406206"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initial length due to replication\n",
    "len(training_build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To correct the replication of offsets for same patients\n",
    "training_build=training_build.groupby(['patientunitstayid','offset'],as_index=False).max().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9536931"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "training_build=training_build.groupby(['patientunitstayid'],as_index=False).apply(pd.DataFrame.sort_values,'offset').reset_index()\n",
    "training_build=training_build.drop(columns=['level_0','level_1'])\n",
    "training_build=training_build.drop(columns=['Norepinephrine','Epinephrine','Dopamine','Dobutamine'])\n",
    "training_build_filtered=training_build.dropna(subset=['paO2_FiO2','platelets_x_1000','total_bilirubin','urinary_creatinine','creatinine','HCO3','pH','paCO2','direct_bilirubin','excess','ast','bun','calcium','GCS_Score'],how='all')\n",
    "training_build_filtered['label']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepsis_labels=pd.read_csv(\"24_hour_sepsis.csv\")\n",
    "sepsis_labels['tsepsis']=sepsis_labels[['tsus','tsofa']].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_build=pd.merge(training_build_filtered,sepsis_labels,how='left',left_on=['patientunitstayid','offset'],right_on=['patientunitstayid','tsepsis'])\n",
    "final_build.label=final_build.flag\n",
    "final_build=final_build.drop(columns=['tsofa','tsus','tsepsis','flag'])\n",
    "#After the initial sepsis=1 flag, all the labels for that patient is given label=1, all before that is 0\n",
    "final_build['label']=final_build.groupby(['patientunitstayid'])['label'].ffill()\n",
    "final_build['label']=final_build['label'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_build.to_csv(\"training_table.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_build_nonneg=final_build.loc[~(final_build['offset']<0)]\n",
    "final_build_nonneg.to_csv(\"training_table_nonneg.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191623"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_build_nonneg['patientunitstayid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids=withoutvital.patientunitstayid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to add vitals as well for these patients\n",
    "withoutvital=pd.read_csv(\"training_table_nonneg.csv\")\n",
    "pids=withoutvital.patientunitstayid.unique()\n",
    "vitals=pd.read_csv(\"vitalPeriodic.csv\",chunksize=10000,usecols=['patientunitstayid','observationoffset','heartrate','respiration','temperature'])\n",
    "chunks=[]\n",
    "def processchunk(chunk):\n",
    "    tomerge=chunk.loc[chunk['patientunitstayid'].isin(pids)]\n",
    "    chunks.append(tomerge)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in vitals:\n",
    "    processchunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_tomerge=pd.concat(chunks,sort=False)\n",
    "vitals_tomerge=vitals_tomerge.rename(columns={'observationoffset':'offset'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vitals_tomerge['patientunitstayid'].unique())\n",
    "del withoutvital\n",
    "del chunks\n",
    "del chunk\n",
    "vitals_tomerge.to_csv(\"vitals_tomerge.csv\",index=False)\n",
    "del vitals_tomerge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the csv files.\n",
    "train1 = dd.read_csv('training_table_nonneg.csv')\n",
    "vitals = dd.read_csv('vitals_tomerge.csv')\n",
    "\n",
    "# Merge the csv files.\n",
    "train2 = dd.merge(train1, vitals, how='outer', on=['patientunitstayid','offset'], npartitions=25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training_with_vitals-00.csv',\n",
       " 'training_with_vitals-01.csv',\n",
       " 'training_with_vitals-02.csv',\n",
       " 'training_with_vitals-03.csv',\n",
       " 'training_with_vitals-04.csv',\n",
       " 'training_with_vitals-05.csv',\n",
       " 'training_with_vitals-06.csv',\n",
       " 'training_with_vitals-07.csv',\n",
       " 'training_with_vitals-08.csv',\n",
       " 'training_with_vitals-09.csv',\n",
       " 'training_with_vitals-10.csv',\n",
       " 'training_with_vitals-11.csv',\n",
       " 'training_with_vitals-12.csv',\n",
       " 'training_with_vitals-13.csv',\n",
       " 'training_with_vitals-14.csv',\n",
       " 'training_with_vitals-15.csv',\n",
       " 'training_with_vitals-16.csv',\n",
       " 'training_with_vitals-17.csv',\n",
       " 'training_with_vitals-18.csv',\n",
       " 'training_with_vitals-19.csv',\n",
       " 'training_with_vitals-20.csv',\n",
       " 'training_with_vitals-21.csv',\n",
       " 'training_with_vitals-22.csv',\n",
       " 'training_with_vitals-23.csv',\n",
       " 'training_with_vitals-24.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the output.\n",
    "train2.to_csv('training_with_vitals-*.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('training_with_vitals*.{}'.format(extension))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('combined_train_vitals.csv', 'w') as outfile:\n",
    "    for in_filename in all_filenames:\n",
    "        with open(in_filename, 'r') as infile:\n",
    "            # if your csv files have headers then you might want to burn a line here with `next(infile)\n",
    "            next(infile)\n",
    "            for line in infile:\n",
    "                outfile.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train=pd.read_csv(\"combined_train_vitals.csv\",chunksize=100000,header=None)\n",
    "df_new=[]\n",
    "for chunk in new_train:\n",
    "    chunk.iloc[:,2:]=chunk.iloc[:,2:].astype('float16')\n",
    "    chunk.iloc[:,:2]=chunk.iloc[:,:2].astype('int32')\n",
    "    df_new.append(chunk)\n",
    "training_data=pd.concat(df_new,sort=False)\n",
    "del df_new\n",
    "del chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['patientunitstayid','offset','paO2_FiO2','platelets_x_1000',\n",
    "      'total_bilirubin','urinary_creatinine','creatinine','HCO3','pH','paCO2',\n",
    "      'direct_bilirubin','excess','ast','bun','calcium','glucose','lactate',\n",
    "      'magnesium','phosphate','potassium','hct','hgb','ptt','wbc','fibrinogen','troponin','GCS_Score','label',\n",
    "      'heartrate','respiration','temperature']\n",
    "\n",
    "training_data.columns=cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#change the datatype of the columns which have max less than 255 and min greater -256 to float8\n",
    "training_data[['GCS_Score','label']]=training_data[['GCS_Score','label']].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=training_data.groupby(['patientunitstayid','offset'],as_index=False).max().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patientunitstayid       int32\n",
       "labresultoffset         int32\n",
       "paO2_FiO2             float16\n",
       "platelets_x_1000      float16\n",
       "total_bilirubin       float16\n",
       "urinary_creatinine    float16\n",
       "creatinine            float16\n",
       "HCO3                  float16\n",
       "pH                    float16\n",
       "paCO2                 float16\n",
       "direct_bilirubin      float16\n",
       "excess                float16\n",
       "ast                   float16\n",
       "bun                   float16\n",
       "calcium               float16\n",
       "glucose               float16\n",
       "lactate               float16\n",
       "magnesium             float16\n",
       "phosphate             float16\n",
       "potassium             float16\n",
       "hct                   float16\n",
       "hgb                   float16\n",
       "ptt                   float16\n",
       "wbc                   float16\n",
       "fibrinogen            float16\n",
       "troponin              float16\n",
       "GCS_Score             float16\n",
       "label                 float16\n",
       "heartrate             float16\n",
       "respiration           float16\n",
       "temperature           float16\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
